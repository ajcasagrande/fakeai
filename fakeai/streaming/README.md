# Unified Streaming Framework

A comprehensive, production-ready streaming infrastructure for FakeAI that provides realistic token-by-token streaming with full metrics, latency simulation, and error handling.

## Overview

The unified streaming framework provides:

- **Multiple stream types**: Chat, completion, audio, realtime
- **Realistic timing**: Model-specific TTFT and ITL simulation
- **Comprehensive metrics**: Basic and detailed streaming metrics
- **Error handling**: Timeout enforcement, cancellation, error recovery
- **Flexible formatting**: SSE, JSON Lines, plain text
- **KV cache integration**: Smart routing with cache awareness
- **Production-ready**: Async/await, thread-safe, well-tested

## Architecture

```

                      StreamManager                          
  - Lifecycle orchestration                                  
  - Timeout enforcement                                      
  - Metrics integration                                      
  - Error handling                                           

                  
        
                           
  
   Generators       Formatters  
 - ChatStream      - SSE        
 - Completion      - JSONLines  
 - Audio           - PlainText  
 - Realtime                     
  
        

                   Supporting Components                      
  - TokenChunker: Text â†’ tokens                              
  - DeltaGenerator: Generate delta objects                   
  - ProgressTracker: Progress monitoring                     
  - AdaptiveChunker: Dynamic chunk sizing                    

```

## Quick Start

### Basic Usage

```python
from fakeai.streaming import StreamManager, StreamType, SSEFormatter

# Create manager
manager = StreamManager()

# Stream a chat response
async for chunk in manager.create_stream(
    stream_type=StreamType.CHAT,
    model="gpt-4o",
    full_response=chat_response,
    endpoint="/v1/chat/completions",
):
    # Format and send chunk
    formatted = SSEFormatter.format(chunk)
    await send_to_client(formatted)

# Send done signal
await send_to_client(SSEFormatter.format_done())
```

### With Full Features

```python
from fakeai.streaming import StreamManager, StreamType
from fakeai.metrics import MetricsTracker
from fakeai.streaming_metrics import StreamingMetricsTracker
from fakeai.latency_profiles import get_latency_manager
from fakeai.kv_cache import SmartRouter, KVCacheMetrics

# Initialize components
metrics_tracker = MetricsTracker()
streaming_metrics_tracker = StreamingMetricsTracker()
latency_manager = get_latency_manager()
kv_cache_router = SmartRouter()
kv_cache_metrics = KVCacheMetrics()

# Create fully-featured manager
manager = StreamManager(
    metrics_tracker=metrics_tracker,
    streaming_metrics_tracker=streaming_metrics_tracker,
    latency_manager=latency_manager,
    kv_cache_router=kv_cache_router,
    kv_cache_metrics=kv_cache_metrics,
    default_timeout_seconds=300.0,
    enable_metrics=True,
    enable_latency_simulation=True,
)

# Stream with all features
async for chunk in manager.create_stream(
    stream_type=StreamType.CHAT,
    model="gpt-4o",
    full_response=response,
    endpoint="/v1/chat/completions",
    prompt_tokens=150,
    temperature=0.7,
    max_tokens=1000,
    client_id="user-123",
):
    yield SSEFormatter.format(chunk)
```

## Components

### StreamManager

Core orchestrator for stream lifecycle management.

**Key Methods:**
- `create_stream()`: Create and manage a streaming response
- `get_stream_status()`: Check stream status
- `get_stream_metrics()`: Get stream metrics
- `cancel_stream()`: Cancel a running stream

**Features:**
- Automatic timeout enforcement
- Metrics tracking integration
- Error handling and recovery
- Resource cleanup

### Generators

Specialized generators for different response types.

#### ChatStreamingGenerator

Streams OpenAI-compatible chat completions.

```python
from fakeai.streaming.generators import ChatStreamingGenerator

generator = ChatStreamingGenerator(latency_manager=latency_manager)

async for chunk in generator.generate(context, chat_response):
    # chunk.data contains OpenAI-format delta
    pass
```

**Output format:**
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion.chunk",
  "created": 1234567890,
  "model": "gpt-4o",
  "choices": [{
    "index": 0,
    "delta": {"content": "Hello"},
    "finish_reason": null
  }]
}
```

#### CompletionStreamingGenerator

Streams legacy text completions.

```python
from fakeai.streaming.generators import CompletionStreamingGenerator

generator = CompletionStreamingGenerator(latency_manager=latency_manager)

async for chunk in generator.generate(context, completion_response):
    # chunk.data contains text delta
    pass
```

### Formatters

Convert chunks into transmission-ready formats.

#### SSEFormatter

Server-Sent Events format (OpenAI standard).

```python
from fakeai.streaming.sse import SSEFormatter

# Format chunk
formatted = SSEFormatter.format(chunk)
# Output: "data: {...}\n\n"

# Format error
error_formatted = SSEFormatter.format_error(error)

# Format done signal
done = SSEFormatter.format_done()
# Output: "data: [DONE]\n\n"
```

#### JSONLinesFormatter

JSON Lines format (one JSON object per line).

```python
from fakeai.streaming.sse import JSONLinesFormatter

formatted = JSONLinesFormatter.format(chunk)
# Output: '{"type":"data","sequence":0,...}\n'
```

#### PlainTextFormatter

Plain text format (content only).

```python
from fakeai.streaming.sse import PlainTextFormatter

formatted = PlainTextFormatter.format(chunk)
# Output: "Hello world"
```

### Chunking Utilities

#### TokenChunker

Splits text into streamable tokens.

```python
from fakeai.streaming.chunking import TokenChunker

chunker = TokenChunker(min_chunk_size=1, max_chunk_size=10)

# Tokenize text
tokens = chunker.tokenize("Hello, world!")
# ['Hello', ',', 'world', '!']

# Chunk text
chunks = chunker.chunk_text("Long text here...")
# ['Long text here', 'more text', ...]
```

#### DeltaGenerator

Generates delta objects for streaming.

```python
from fakeai.streaming.chunking import DeltaGenerator

generator = DeltaGenerator()

# Chat delta
delta = generator.generate_chat_delta(
    role="assistant",
    content="Hello",
)

# Completion delta
delta = generator.generate_completion_delta(text="Hello")

# Audio delta
delta = generator.generate_audio_delta(
    audio_chunk=b"audio_data",
    transcript_delta="Hello",
)
```

#### ProgressTracker

Tracks streaming progress.

```python
from fakeai.streaming.chunking import ProgressTracker

tracker = ProgressTracker(total_tokens=100)

# Record chunks
tracker.record_chunk(token_count=10)

# Get progress
progress = tracker.progress_percentage()  # 10.0
tps = tracker.tokens_per_second()  # 50.0
ttfc = tracker.time_to_first_chunk_ms()  # 150.0
```

#### AdaptiveChunker

Dynamically adjusts chunk size for optimal throughput.

```python
from fakeai.streaming.chunking import AdaptiveChunker

chunker = AdaptiveChunker(
    initial_chunk_size=5,
    target_chunk_delay_ms=50.0,
)

# Adjust based on observed delays
chunker.adjust_chunk_size(actual_delay_ms=40.0)

# Get current size
size = chunker.get_chunk_size()
```

## Base Types

### StreamContext

Configuration for stream generation.

```python
from fakeai.streaming.base import StreamContext, StreamType

context = StreamContext(
    stream_id="stream-123",
    stream_type=StreamType.CHAT,
    model="gpt-4o",
    endpoint="/v1/chat/completions",
    timeout_seconds=300.0,
    chunk_delay_seconds=0.05,
    prompt_tokens=100,
    temperature=0.7,
)
```

### StreamChunk

A single unit of streaming data.

```python
from fakeai.streaming.base import StreamChunk

chunk = StreamChunk(
    chunk_id="chunk-1",
    sequence_number=0,
    data={"content": "Hello"},
    chunk_type="chat.completion.chunk",
    token_count=1,
    is_first=True,
    is_last=False,
)
```

### StreamMetrics

Metrics collected during stream lifecycle.

```python
metrics = StreamMetrics(
    stream_id="stream-123",
    start_time=time.time(),
)

# Metrics are automatically tracked
metrics.total_chunks  # 50
metrics.total_tokens  # 200
metrics.time_to_first_chunk_ms()  # 150.0
metrics.tokens_per_second()  # 100.0
```

### StreamError

Error information with recovery options.

```python
from fakeai.streaming.base import StreamError, ErrorSeverity

error = StreamError(
    error_code="stream_timeout",
    message="Stream exceeded 300s timeout",
    severity=ErrorSeverity.ERROR,
    recoverable=True,
    retry_after_seconds=1.0,
)
```

## Integration with Existing Systems

### Metrics Integration

```python
# The streaming framework automatically integrates with:

# 1. Basic MetricsTracker (fakeai.metrics)
#    - Tracks request/response rates
#    - Tracks token throughput
#    - Tracks error rates

# 2. StreamingMetricsTracker (fakeai.streaming_metrics)
#    - Detailed per-stream metrics
#    - Token timing analysis
#    - Quality metrics (jitter, smoothness)
#    - Client behavior tracking

# 3. LatencyProfileManager (fakeai.latency_profiles)
#    - Realistic model-specific TTFT
#    - Realistic ITL per model
#    - Load-aware timing

# 4. KV Cache System (fakeai.kv_cache)
#    - Smart routing
#    - Cache hit tracking
#    - Speedup measurement
```

### Error Handling

```python
from fakeai.streaming.base import (
    StreamTimeoutException,
    StreamCancelledException,
    StreamGenerationException,
)

try:
    async for chunk in manager.create_stream(...):
        yield chunk
except StreamTimeoutException as e:
    # Handle timeout
    logger.error(f"Stream timeout: {e.stream_id}")
except StreamCancelledException as e:
    # Handle cancellation
    logger.info(f"Stream cancelled: {e.stream_id}")
except StreamGenerationException as e:
    # Handle generation error
    logger.error(f"Generation failed: {e.error.message}")
```

## Testing

The framework includes 91 comprehensive tests covering:

- Base types and protocols (19 tests)
- Chunking utilities (26 tests)
- SSE formatters (18 tests)
- Stream manager (11 tests)
- Generators (8 tests)
- Integration scenarios (9 tests)

Run tests:

```bash
pytest tests/streaming/ -v
```

## Performance

### Timing Characteristics

- **TTFT (Time to First Token)**: Model-specific, 90-900ms
- **ITL (Inter-Token Latency)**: Model-specific, 5-40ms
- **Throughput**: 20-200 tokens/second (model-dependent)
- **Overhead**: <5% memory overhead, <1ms CPU per chunk

### Scalability

- **Concurrent streams**: Thousands (limited by system resources)
- **Memory per stream**: ~10KB base + chunk data
- **CPU per stream**: Minimal (mostly I/O wait)

## Advanced Usage

### Custom Generators

```python
from fakeai.streaming.base import StreamingGenerator, StreamContext, StreamChunk
from typing import AsyncIterator, Any

class CustomStreamingGenerator(StreamingGenerator):
    async def generate(
        self,
        context: StreamContext,
        full_response: Any,
    ) -> AsyncIterator[StreamChunk]:
        # Your custom streaming logic
        for i, data in enumerate(process_response(full_response)):
            chunk = StreamChunk(
                chunk_id=f"{context.stream_id}-{i}",
                sequence_number=i,
                data=data,
                is_first=(i == 0),
                is_last=(i == len(data) - 1),
            )
            yield chunk

# Use custom generator
manager = StreamManager()
custom_generator = CustomStreamingGenerator()

async for chunk in manager.create_stream(
    stream_type=StreamType.CHAT,
    full_response=response,
    generator=custom_generator,
):
    yield chunk
```

### Custom Formatters

```python
from fakeai.streaming.base import ChunkFormatter, StreamChunk, StreamError

class CustomFormatter(ChunkFormatter):
    def format(self, chunk: StreamChunk) -> str:
        # Your custom format
        return f"CHUNK:{chunk.data}\n"

    def format_error(self, error: StreamError) -> str:
        return f"ERROR:{error.message}\n"

    def format_done(self) -> str:
        return "DONE\n"
```

## Troubleshooting

### Streams not timing out

Ensure `timeout_seconds` is set and reasonable:

```python
async for chunk in manager.create_stream(
    ...,
    timeout_seconds=60.0,  # Explicit timeout
):
    pass
```

### Metrics not tracking

Verify metrics are enabled and trackers are provided:

```python
manager = StreamManager(
    metrics_tracker=metrics_tracker,
    enable_metrics=True,  # Must be True
)
```

### Slow streaming

Check chunk delay and latency simulation settings:

```python
async for chunk in manager.create_stream(
    ...,
    chunk_delay_seconds=0.01,  # Faster chunks
    enable_latency_simulation=False,  # Disable for tests
):
    pass
```

## Future Enhancements

- [ ] Audio streaming (full implementation)
- [ ] Realtime bidirectional streaming (full implementation)
- [ ] Image streaming
- [ ] Binary data streaming
- [ ] Compression support
- [ ] Client-side backpressure handling
- [ ] Rate limiting per stream
- [ ] Priority-based streaming

## License

Apache-2.0

## Contributing

See the main FakeAI CONTRIBUTING.md for guidelines.
